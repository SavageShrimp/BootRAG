{
  "id": "conv-1737955449239",
  "lastModified": 1737956600051,
  "messages": [
    {
      "id": 1737955529105,
      "role": "user",
      "content": "I need to modify MyCustomHandler which recieves tokens asynchrounously and pass them on to generate_response which is a flask endpoint function\n\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        try:\n            for response_chunk in pipeline.generate_response(text):\n                yield jsonify({'text': response_chunk}).data + b'\\n'\n        except Exception as e:\n            yield jsonify({'error': str(e)}).data + b'\\n'\n            raise\n\n    return app.response_class(stream_response(), mimetype='application/json')\nclass MyCustomHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(f\"My custom handler, token: {token}\")\n\n    def on_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n    def raise_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n"
    },
    {
      "id": 1737955529111,
      "role": "assistant",
      "content": "To integrate your `MyCustomHandler` with the Flask `generate_response` endpoint, you need to ensure that the tokens generated by the pipeline are passed to the Flask endpoint and streamed back as a response. Here's how you can achieve this:\n\n1. Modify the `generate_response` function to use the `MyCustomHandler` to receive tokens.\n2. Ensure that the `MyCustomHandler` correctly passes the tokens to the Flask endpoint.\n3. Stream the tokens back as a response.\n\nHere's a step-by-step implementation:\n\n### Step 1: Update `generate_response` to Use `MyCustomHandler`\n\nYou need to ensure that `MyCustomHandler` is used to handle the tokens generated by the pipeline. This can be done by passing the handler to the pipeline when generating the response.\n\n```python\nfrom flask import Flask, request, jsonify, Response\nfrom flask import stream_with_context\n\napp = Flask(__name__)\n\nclass MyCustomHandler(BaseCallbackHandler):\n    def __init__(self):\n        self.tokens = []\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.tokens.append(token)\n\n    def on_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n    def raise_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        # Initialize the custom handler\n        handler = MyCustomHandler()\n\n        # Generate response using the pipeline and the custom handler\n        for response_chunk in pipeline.generate_response(text, handler=handler):\n            yield jsonify({'text': response_chunk}).data + b'\\n'\n\n        # Yield any remaining tokens after the pipeline has finished\n        for token in handler.tokens:\n            yield jsonify({'text': token}).data + b'\\n'\n\n    return app.response_class(stream_response(), mimetype='application/json')\n```\n\n### Step 2: Modify the Pipeline to Accept a Handler\n\nEnsure that your `pipeline.generate_response` method can accept a callback handler. Here's a simplified example of how you might modify the pipeline to accept a handler:\n\n```python\nclass Pipeline:\n    def generate_response(self, text, handler=None):\n        # Simulate token generation\n        for token in ['token1', 'token2', 'token3']:\n            if handler:\n                handler.on_llm_new_token(token)\n            yield token\n```\n\n### Step 3: Ensure Proper Streaming\n\nThe `stream_response` function in the Flask endpoint ensures that each token is streamed back to the client as soon as it is generated. This is achieved by yielding each token as a JSON response.\n\n### Full Example\n\nHere's the full example, combining all the pieces:\n\n```python\nfrom flask import Flask, request, jsonify, Response\nfrom flask import stream_with_context\n\napp = Flask(__name__)\n\nclass MyCustomHandler(BaseCallbackHandler):\n    def __init__(self):\n        self.tokens = []\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.tokens.append(token)\n\n    def on_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n    def raise_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\nclass Pipeline:\n    def generate_response(self, text, handler=None):\n        # Simulate token generation\n        for token in ['token1', 'token2', 'token3']:\n            if handler:\n                handler.on_llm_new_token(token)\n            yield token\n\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        # Initialize the custom handler\n        handler = MyCustomHandler()\n\n        # Generate response using the pipeline and the custom handler\n        for response_chunk in pipeline.generate_response(text, handler=handler):\n            yield jsonify({'text': response_chunk}).data + b'\\n'\n\n        # Yield any remaining tokens after the pipeline has finished\n        for token in handler.tokens:\n            yield jsonify({'text': token}).data + b'\\n'\n\n    return app.response_class(stream_response(), mimetype='application/json')\n\nif __name__ == '__main__':\n    pipeline = Pipeline()\n    app.run(debug=True)\n```\n\nThis setup ensures that the `MyCustomHandler` correctly handles the tokens generated by the pipeline and streams them back to the client in real-time."
    },
    {
      "id": 1737955823364,
      "role": "user",
      "content": "But llm_evaluator already uses MyCustomCallback\n\nimport json\nimport os\nfrom torch import cuda\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom rag_callback import CustomCallbackHandler, MyCustomHandler\nfrom database_to_rag import DatabaseToRagConverter\nfrom embeddings import embeddings\nimport logging\n\n# Load configuration from JSON file\nconfig_path = \"config.json\"\n\nif not os.path.exists(config_path):\n    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\nwith open(config_path, 'r') as config_file:\n    config = json.load(config_file)\n\nllm_model_path = config[\"llm_model_path\"]\nn_gpu_layers = config[\"n_gpu_layers\"]\nn_batch = config[\"n_batch\"]\nn_ctx = config[\"n_ctx\"]\nf16kv = config[\"f16kv\"]\n\n# Embeddings configuration\nembed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\nembed_model = HuggingFaceEmbeddings(\n    model_name=embed_model_id,\n    model_kwargs={'device': device},\n    encode_kwargs={'device': device, 'batch_size': 32}\n)\n\n# LLM configuration\n#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\ncallback_manager = CallbackManager([MyCustomHandler()])\n\nllm = LlamaCpp(\n    model_path=llm_model_path,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    n_ctx=n_ctx,\n    f16kv=f16kv,\n    max_tokens=2000,\n    callback_manager=callback_manager,\n    verbose=False,\n)\n\nlogging.basicConfig(level=logging.DEBUG)\n\nclass LlmEvaluator:\n    def __init__(self, embed_model, llm):\n        self.embed_model = embed_model\n        self.llm = llm\n\n    def evaluate_data(self, text, db_path):\n        # Embed the text\n        print(\"***\", text, \"***\")\n        embeddings = self.embed_model.embed_documents([text])\n\n        #allbacks = [CustomCallbackHandler()]\n        callbacks = [MyCustomHandler()]\n        manager = CallbackManager(handlers=callbacks)\n        #self.llm = LlamaCpp( ... callbacks=manager ... )\n\n        # And when calling the model:\n        #with manager:\n            # Generate a response using the LLM\n        response = self.llm.invoke(text)\n\n        # Close the database connection\n        converter = DatabaseToRagConverter(db_path)\n        converter.close()\n\n        return embeddings, response\n\n"
    },
    {
      "id": 1737955823370,
      "role": "assistant",
      "content": "Given that you are already using `MyCustomHandler` within your `LlmEvaluator` class and you want to integrate this with your Flask endpoint, you need to ensure that the `MyCustomHandler` is correctly capturing the tokens and passing them to the Flask `generate_response` endpoint. \n\nHere's a step-by-step approach to achieve this:\n\n1. **Ensure `MyCustomHandler` Captures Tokens Correctly:**\n   - Modify `MyCustomHandler` to store tokens in a list or any other data structure that can be accessed outside its scope.\n\n2. **Pass `MyCustomHandler` to the LLM and Capture Tokens:**\n   - Use `MyCustomHandler` to capture the tokens generated by the LLM.\n\n3. **Stream Tokens Back to the Flask Endpoint:**\n   - Use the tokens captured by `MyCustomHandler` to stream the response back to the client.\n\n### Step 1: Modify `MyCustomHandler` to Capture Tokens\n\nEnsure that `MyCustomHandler` stores the tokens in a list or any other data structure that can be accessed outside its scope.\n\n```python\nclass MyCustomHandler(BaseCallbackHandler):\n    def __init__(self):\n        super().__init__()\n        self.tokens = []\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.tokens.append(token)\n\n    def get_tokens(self):\n        return self.tokens\n\n    def on_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n    def raise_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n```\n\n### Step 2: Modify `LlmEvaluator` to Use `MyCustomHandler`\n\nEnsure that `MyCustomHandler` is used to capture the tokens generated by the LLM.\n\n```python\nclass LlmEvaluator:\n    def __init__(self, embed_model, llm):\n        self.embed_model = embed_model\n        self.llm = llm\n\n    def evaluate_data(self, text, db_path):\n        # Embed the text\n        print(\"***\", text, \"***\")\n        embeddings = self.embed_model.embed_documents([text])\n\n        # Use MyCustomHandler to capture tokens\n        callbacks = [MyCustomHandler()]\n        manager = CallbackManager(handlers=callbacks)\n\n        # Generate a response using the LLM\n        with manager:\n            response = self.llm.invoke(text)\n\n        # Get the tokens captured by MyCustomHandler\n        handler = manager.handlers[0]  # Assuming only one handler\n        tokens = handler.get_tokens()\n\n        # Close the database connection\n        converter = DatabaseToRagConverter(db_path)\n        converter.close()\n\n        return embeddings, response, tokens\n```\n\n### Step 3: Integrate Tokens into Flask `generate_response`\n\nModify the Flask `generate_response` endpoint to use the tokens captured by `MyCustomHandler`.\n\n```python\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        # Use MyCustomHandler to capture tokens\n        callbacks = [MyCustomHandler()]\n        manager = CallbackManager(handlers=callbacks)\n\n        # Generate a response using the LLM\n        with manager:\n            response = llm.invoke(text)\n\n        # Get the tokens captured by MyCustomHandler\n        handler = manager.handlers[0]  # Assuming only one handler\n        tokens = handler.get_tokens()\n\n        # Stream the tokens back to the client\n        for token in tokens:\n            yield jsonify({'text': token}).data + b'\\n'\n\n    return app.response_class(stream_response(), mimetype='application/json')\n```\n\n### Full Example\n\nHere's the complete updated code:\n\n```python\nimport json\nimport os\nfrom torch import cuda\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom rag_callback import CustomCallbackHandler, MyCustomHandler\nfrom database_to_rag import DatabaseToRagConverter\nfrom embeddings import embeddings\nimport logging\n\n# Load configuration from JSON file\nconfig_path = \"config.json\"\n\nif not os.path.exists(config_path):\n    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\nwith open(config_path, 'r') as config_file:\n    config = json.load(config_file)\n\nllm_model_path = config[\"llm_model_path\"]\nn_gpu_layers = config[\"n_gpu_layers\"]\nn_batch = config[\"n_batch\"]\nn_ctx = config[\"n_ctx\"]\nf16kv = config[\"f16kv\"]\n\n# Embeddings configuration\nembed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\nembed_model = HuggingFaceEmbeddings(\n    model_name=embed_model_id,\n    model_kwargs={'device': device},\n    encode_kwargs={'device': device, 'batch_size': 32}\n)\n\n# LLM configuration\ncallback_manager = CallbackManager([MyCustomHandler()])\n\nllm = LlamaCpp(\n    model_path=llm_model_path,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    n_ctx=n_ctx,\n    f16kv=f16kv,\n    max_tokens=2000,\n    callback_manager=callback_manager,\n    verbose=False,\n)\n\nlogging.basicConfig(level=logging.DEBUG)\n\nclass MyCustomHandler(BaseCallbackHandler):\n    def __init__(self):\n        super().__init__()\n        self.tokens = []\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.tokens.append(token)\n\n    def get_tokens(self):\n        return self.tokens\n\n    def on_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\n    def raise_error(self, error: str) -> None:\n        print(f\"Error: {error}\")\n\nclass LlmEvaluator:\n    def __init__(self, embed_model, llm):\n        self.embed_model = embed_model\n        self.llm = llm\n\n    def evaluate_data(self, text, db_path):\n        # Embed the text\n        print(\"***\", text, \"***\")\n        embeddings = self.embed_model.embed_documents([text])\n\n        # Use MyCustomHandler to capture tokens\n        callbacks = [MyCustomHandler()]\n        manager = CallbackManager(handlers=callbacks)\n\n        # Generate a response using the LLM\n        with manager:\n            response = self.llm.invoke(text)\n\n        # Get the tokens captured by MyCustomHandler\n        handler = manager.handlers[0]  # Assuming only one handler\n        tokens = handler.get_tokens()\n\n        # Close the database connection\n        converter = DatabaseToRagConverter(db_path)\n        converter.close()\n\n        return embeddings, response, tokens\n\n# Initialize the evaluator\nevaluator = LlmEvaluator(embed_model, llm)\n\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        # Use MyCustomHandler to capture tokens\n        callbacks = [MyCustomHandler()]\n        manager = CallbackManager(handlers=callbacks)\n\n        # Generate a response using the LLM\n        with manager:\n            evaluator.evaluate_data(text, db_path='your_database_path')  # Replace with actual db_path\n\n        # Get the tokens captured by MyCustomHandler\n        handler = manager.handlers[0]  # Assuming only one handler\n        tokens = handler.get_tokens()\n\n        # Stream the tokens back to the client\n        for token in tokens:\n            yield jsonify({'text': token}).data + b'\\n'\n\n    return app.response_class(stream_response(), mimetype='application/json')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n### Explanation\n\n1. **MyCustomHandler**: Captures tokens and stores them in a list.\n2. **LlmEvaluator**: Uses `MyCustomHandler` to capture tokens during the generation process.\n3. **Flask Endpoint**: Streams the captured tokens back to the client.\n\nEnsure you replace `'your_database_path'` with the actual path to your database. This setup ensures that the tokens are correctly captured and streamed back to the client in real-time."
    },
    {
      "id": 1737956429022,
      "role": "user",
      "content": "Debugging middleware caught exception in streamed response at a point where response headers were already sent.\nTraceback (most recent call last):\n  File \"/home/mark/src/llama_rag/.venv/lib/python3.10/site-packages/werkzeug/wsgi.py\", line 256, in __next__\n    return self._next()\n  File \"/home/mark/src/llama_rag/.venv/lib/python3.10/site-packages/werkzeug/wrappers/response.py\", line 32, in _iter_encoded\n    for item in iterable:\n  File \"/home/mark/src/llama_rag/.venv/lib/python3.10/site-packages/flask/helpers.py\", line 125, in generator\n    yield from gen\n  File \"/mnt/source/src/llama_rag/step2/api.py\", line 108, in stream_response\n    with manager:\nAttributeError: __enter__\n"
    },
    {
      "id": 1737956429030,
      "role": "assistant",
      "content": "The error you're encountering, `AttributeError: __enter__`, suggests that the object being used in the `with` statement does not support the context manager protocol (i.e., it does not implement `__enter__` and `__exit__` methods). In this case, it appears to be the `manager` object from `CallbackManager`.\n\nTo resolve this issue, you need to ensure that `CallbackManager` is correctly used as a context manager. It seems that the `CallbackManager` object should be used directly in the `with` statement without wrapping it in another context manager.\n\n### Revised Code\n\nHere's the revised code that correctly uses the `CallbackManager` as a context manager:\n\n```python\nfrom flask import Flask, request, jsonify, Response\nfrom flask import stream_with_context\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom rag_callback import CustomCallbackHandler, MyCustomHandler\nfrom database_to_rag import DatabaseToRagConverter\nfrom embeddings import embeddings\nimport logging\n\n# Load configuration from JSON file\nconfig_path = \"config.json\"\n\nif not os.path.exists(config_path):\n    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\nwith open(config_path, 'r') as config_file:\n    config = json.load(config_file)\n\nllm_model_path = config[\"llm_model_path\"]\nn_gpu_layers = config[\"n_gpu_layers\"]\nn_batch = config[\"n_batch\"]\nn_ctx = config[\"n_ctx\"]\nf16kv = config[\"f16kv\"]\n\n# Embeddings configuration\nembed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\nembed_model = HuggingFaceEmbeddings(\n    model_name=embed_model_id,\n    model_kwargs={'device': device, 'f1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111"
    }
  ]
}