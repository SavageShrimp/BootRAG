{
  "id": "conv-1737973621855",
  "lastModified": 1737975399243,
  "messages": [
    {
      "id": 1737974002150,
      "role": "user",
      "content": "from flask import Flask, request, jsonify, stream_with_context\nfrom rag_pipeline import RagPipeline\nfrom langchain_community.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom rag_callback import CustomCallbackHandler, MyCustomHandler\nfrom embeddings import embeddings\nimport json\nimport os\nimport queue\nimport logging\nfrom logging.config import dictConfig\nimport torch\n\n# Load configuration from JSON file\nconfig_path = \"config.json\"\n\nif not os.path.exists(config_path):\n    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\nwith open(config_path, 'r') as config_file:\n    config = json.load(config_file)\n\nllm_model_path = config.get(\"llm_model_path\")\nn_gpu_layers = config.get(\"n_gpu_layers\", 0)\nn_batch = config.get(\"n_batch\", 8)\nn_ctx = config.get(\"n_ctx\", 5000)\nf16kv = config.get(\"f16kv\", False)\ndb_path = config.get(\"db_path\")\nfile_path = config.get(\"file_path\")\nport = config.get(\"port\", 5000)\n\n# Ensure f16kv is a boolean\nif isinstance(f16kv, str):\n    f16kv = f16kv.lower() in ['true', '1', 't']\n\n# Configure logging\ndictConfig({\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'standard': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'standard',\n            'level': 'DEBUG'\n        },\n    },\n    'loggers': {\n        '': {  # root logger\n            'handlers': ['console'],\n            'level': 'DEBUG',\n            'propagate': True\n        }\n    }\n})\n\n# Initialize embeddings\nembed_model = embeddings.get_embeddings()\n\n# Initialize LLM\ncallback_manager = CallbackManager([MyCustomHandler()])\n\nllm = LlamaCpp(\n    model_path=llm_model_path,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    n_ctx=n_ctx,\n    f16kv=f16kv,\n    streaming=True,\n    callback_manager=callback_manager,\n    verbose=False\n)\n\n# Initialize the RagPipeline\npipeline = RagPipeline(db_path, llm, embed_model)\n\napp = Flask(__name__)\n\n@app.route('/process', methods=['POST'])\ndef process_file():\n    data = request.get_json()\n    name = data.get('name')\n    text = data.get('text')\n    print(name, \" \", text)\n    try:\n        pipeline.store_chunks(name, text)\n        processed_text = f'Processed text chunks - {name}'\n        return jsonify({'text': processed_text})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        try:\n            # Use the pipeline to process the text and get the response generator\n            generator = pipeline.generate_response(text)\n\n            # Use a queue to collect tokens\n            token_queue = Queue()\n\n            # Start a thread to process tokens\n            def token_handler():\n                for token in generator:\n                    token_queue.put(token)\n                token_queue.put(None)  # Signal end of stream\n\n            thread = Thread(target=token_handler)\n            thread.start()\n\n            while True:\n                token = token_queue.get()\n                if token is None:\n                    break\n                yield jsonify({'token': token}).data + b'\\n'\n\n            thread.join()\n        except Exception as e:\n            yield jsonify({'error': str(e)}).data + b'\\n'\n            raise\n\n    return app.response_class(stream_response(), mimetype='application/json')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n  File \"/home/mark/src/llama_rag/.venv/lib/python3.10/site-packages/flask/helpers.py\", line 125, in generator\n    yield from gen\n  File \"/mnt/source/src/llama_rag/step2/api.py\", line 108, in stream_response\n    token_queue = Queue()\nNameError: name 'Queue' is not defined\n"
    },
    {
      "id": 1737974002158,
      "role": "assistant",
      "content": "<think>\nAlright, I'm trying to figure out why this Flask app is giving me a NameError saying that 'Queue' is not defined. Let me look through the code to see where Queue is being used.\n\nOkay, in the stream_response function, there's this line: token_queue = Queue(). Hmm, I remember that Queue is part of the queue module. So maybe the code is missing an import statement for Queue.\n\nLooking at the imports at the top, I see 'import queue' but no 'from queue import Queue'. Oh, right, because Queue is a class in the queue module, so to use it directly, I need to import it explicitly.\n\nSo the issue is that the code is trying to create a Queue instance without importing it. The error occurs because Python doesn't recognize Queue as a defined name. To fix this, I should add 'from queue import Queue' to the imports.\n\nI'll check if there are any other places where Queue is used. It seems like it's only used here. So adding that import should solve the problem.\n\nAnother thing I notice is that the code uses Thread, but I don't see an import for Thread from the threading module. So I should also add 'from threading import Thread' to avoid another NameError.\n\nSo, to summarize, the two missing imports are Queue and Thread. Adding them should fix the NameError.\n</think>\n\nThe error occurs because the `Queue` class is not imported. You need to import `Queue` from the `queue` module. Here's how to fix it:\n\n```python\n# Add this import at the top of your file\nfrom queue import Queue\nfrom threading import Thread\n```\n\nThen, in your code, replace `Queue()` with `Queue()`.\n\nHere's the corrected version of your `stream_response` function:\n\n```python\n@app.route('/generate', methods=['POST'])\ndef generate_response():\n    @stream_with_context\n    def stream_response():\n        data = request.get_json()\n        text = data.get('text')\n\n        try:\n            # Use the pipeline to process the text and get the response generator\n            generator = pipeline.generate_response(text)\n\n            # Use a queue to collect tokens\n            token_queue = Queue()\n\n            # Start a thread to process tokens\n            def token_handler():\n                for token in generator:\n                    token_queue.put(token)\n                token_queue.put(None)  # Signal end of stream\n\n            thread = Thread(target=token_handler)\n            thread.start()\n\n            while True:\n                token = token_queue.get()\n                if token is None:\n                    break\n                yield jsonify({'token': token}).data + b'\\n'\n\n            thread.join()\n        except Exception as e:\n            yield jsonify({'error': str(e)}).data + b'\\n'\n            raise\n\n    return app.response_class(stream_response(), mimetype='application/json')\n```\n\nMake sure to add the imports at the top of your file:\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n```\n\nThis should resolve the `NameError` you're encountering."
    },
    {
      "id": 1737975399243,
      "role": "user",
      "content": "app.js:158 Uncaught (in promise) ReferenceError: decoder is not defined\n    at processChunk (app.js:158:38)"
    }
  ]
}